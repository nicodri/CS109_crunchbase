{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use the data provided by the Crunchbase API as it gathers up-to-date information about hundred of thousands of companies and it's considered as the online start ups database reference. \n",
    "\n",
    "The API accessible on the following link https://data.crunchbase.com/ is well documented and we only needed to ask for a user key. However, we had to work hard on understanding its intern architecture and on how to manipulate it for our purposes.\n",
    "\n",
    "The API stores classes corresponding to each object we are manipulating (Company, Locations, People...) but also for the relationship between them. After digging deeper into attributes accessible on the API, we found out that the best way to retrieve the data was by relationship on each company. That's why, for each company, we need to do several API calls, one for each relationship we consider. We then realized how important it was to make your code sleep in between the calls as the API put a cap on the authorized bandwith per user key.\n",
    "\n",
    "First, we scrapped all the organization names and short description from the REST API in the list 'organizations_tot', which gave us 408 422 organizations and took 3h30. We had to sleep in between our API calls because the server could identify our API key and limit our call. This file is saved as a json in 'tmp/organizations_json.json', we mapped the company permalink (ie name in the REST API naming) with its index in the previous list for an easy access.\n",
    "\n",
    "Second, we used the excel API to retrieve a list of companies names for which crunchbaes contains enough information. We realized indeed that there were too many missing values on the companies provided by the REST API so a first filtering was required; also it would take too much time to retrieve these information for all the companies because of the API cap on the calls. The Excel API provides a good one because all the companies listed there contain at least information about categories, offices location and funding information. More important is that it stores the fundation date. We chose to build models on the companies funded before 2010 to be sure to have an horizon large enough to determine its success. The excel API gave us a list of 61 399 companies we reduced to 25 000 with the filter on the foundation date. For time matters, we chose to retrieve **8 000 companies**. This list is stored in the file 'organizations_xlsx.json'\n",
    "\n",
    "Third, we had to retrieve the features we wanted from the REST API. It is is built around a relationship graph between the different instance (company, person, funding...). We went through the different relationships and instance to store only the one we care about. This provided the list 'relationships', which stores the relationships we consider, and the dictionary 'relationships_infos' which stores for each instance type the attributes we consider. Once this preliminary work has been done, we wrote the multiple calls needed to store these attributes for each organization. The json is stored in 'organizations_dict.json'. It's a dictionary by permalink with attributes corresponding to the features we care about (some atributes are also dictionaries). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Scraping Data](#.-Scraping-Data)\n",
    "* [1. Organization List](#1.-Organization-List)\n",
    "* [2. Excel API](#2.-Excel-API)\n",
    "* [3. Relationships](#3.-Relationships) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def get_json(url):\n",
    "    r_ = requests.get(url)\n",
    "    try:\n",
    "        return r_.json()\n",
    "    except ValueError:\n",
    "        time.sleep(1)\n",
    "        return get_json(url)\n",
    "\n",
    "user_key = '1a49911e9216b8037dc430f811fd5b6b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Organizations List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we expose how to call the API and the structure we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://api.crunchbase.com/v/3/organizations?user_key={}'.format(user_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of companies is the value for the keys items of the data of the request. The information to retrieve is then in the values of the key 'properties'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'properties': {u'api_path': u'organizations/dcode-economic-and-financial-consulting',\n",
       "  u'city_name': None,\n",
       "  u'country_code': None,\n",
       "  u'created_at': 1449800531,\n",
       "  u'domain': u'dcodeefc.com',\n",
       "  u'facebook_url': None,\n",
       "  u'homepage_url': u'http://dcodeefc.com/',\n",
       "  u'linkedin_url': None,\n",
       "  u'name': u'Dcode Economic and Financial Consulting',\n",
       "  u'permalink': u'dcode-economic-and-financial-consulting',\n",
       "  u'primary_role': u'company',\n",
       "  u'profile_image_url': u'http://public.crunchbase.com/t_api_images/v1449800495/yijj0vm1vxh0vsp7gmni.png',\n",
       "  u'region_name': None,\n",
       "  u'short_description': u'Dcode EFC is a growing economic and financial consulting firm currently operating in Egypt.',\n",
       "  u'twitter_url': None,\n",
       "  u'updated_at': 1449800559,\n",
       "  u'web_path': u'organization/dcode-economic-and-financial-consulting'},\n",
       " u'type': u'OrganizationSummary',\n",
       " u'uuid': u'd0112f89fa19cba8e9f2d1bea73dc27d'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['data']['items'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show here how to retrieve the companies from the enxt page as each page contains only 100 items. This would be done by a manual change of the 'next_page_url' until we reached the totatl number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'current_page': 1,\n",
       " u'items_per_page': 100,\n",
       " u'next_page_url': u'https://api.crunchbase.com/v/3/organizations?page=2',\n",
       " u'number_of_pages': 4159,\n",
       " u'prev_page_url': None,\n",
       " u'sort_order': u'created_at DESC',\n",
       " u'total_items': 415865}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['data']['paging']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main script of this part. We iterate over the pages and then append the list of item to the current organizations list. It took around **5 hours** to run (because of the API cap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page  3400  length is  3300\n",
      "page  3500  length is  13300\n",
      "page  3600  length is  23300\n",
      "page  3700  length is  33300\n",
      "page  3800  length is  43300\n",
      "page  3900  length is  53300\n",
      "page  4000  length is  63300\n"
     ]
    }
   ],
   "source": [
    "organizations = []\n",
    "for i in xrange(1, 4087):\n",
    "    link = 'https://api.crunchbase.com/v/3/organizations?page={}&user_key={}'.format(i, user_key)\n",
    "    if i%100 == 0:\n",
    "        print 'page ', i, ' length is ',len(organizations)\n",
    "    r_json = get_json(link)\n",
    "    organizations += r_json['data']['items']\n",
    "    time.sleep(0.1)\n",
    "\n",
    "with open('tmp/organizations_json.json', 'w') as f:\n",
    "    json.dump(organizations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map from the permalink of a company to its index in the list organizations_tot\n",
    "permalink2index = {organizations_tot[i]['properties']['permalink']: i for i in xrange(len(organizations_tot))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieved in total 400 000 companies and stored a mapping from the permalink to the index in the organisations list. (The cells results are not the global ones because we needed to rerun some of them because of API calls error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Excel API\n",
    "\n",
    "The excel companies spreadsheet was retrieved directly from the website of crunchbase API. We load it with the python library openpyxl and iterate through it to build the list of companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/openpyxl/workbook/names/named_range.py:124: UserWarning: Discarded range with reserved name\n",
      "  warnings.warn(\"Discarded range with reserved name\")\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "wb = load_workbook(filename='test.xlsx', read_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go into the sheet 'Companies' of the excel file and retrieve only the company name with tis founded date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main script of this part where we retrieve the list of companies from the excel API as a dictionary under the format 'permalink' : 'founded month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wb['Companies']\n",
    "# List of the columns names to retrieve the index of the columns we care about\n",
    "rows = enumerate(wc.rows)\n",
    "\n",
    "# Removing the first element because it is not in the good format\n",
    "row = rows.next()\n",
    "for i, val in enumerate(row[1]):\n",
    "    if val.value == 'founded_month':\n",
    "        month_col = i\n",
    "    if val.value == 'permalink':\n",
    "        permalink_col = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61398"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary with (key, value) = (permalink, founded month)\n",
    "organizations_xlsx = {}\n",
    "rows = enumerate(wc.rows)\n",
    "\n",
    "# Removing the header of the generator\n",
    "row = rows.next()\n",
    "\n",
    "for row in rows:\n",
    "    # Get rid of the first 14 characters which are '/organizations/'\n",
    "    organizations_xlsx[row[1][permalink_col].value[14:]] = row[1][month_col].value\n",
    "len(organizations_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the list\n",
    "# with open('tmp/organizations_xlsx.json', 'w') as f:\n",
    "#   json.dump(organizations_xlsx, f)\n",
    "# Loading the list (not to run again the code above)\n",
    "with open('tmp/organizations_xlsx.json', 'r') as f:\n",
    "    organizations_xlsx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies funded before 2011:  25171\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the list of the company founded before 2011\n",
    "import collections\n",
    "\n",
    "# This counter counts the number of companies founded for each month\n",
    "companies_years = collections.Counter(organizations_xlsx.values())\n",
    "print 'Number of companies funded before 2011: ',sum([val for k, val in companies_years.iteritems() if k and int(k[:4]) < 2011])\n",
    "\n",
    "# Dictionnary extracted from oraganizations_xlsx\n",
    "focus_companies = {k:v for k,v in organizations_xlsx.iteritems() if v and int(v[:4]) < 2011}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we retrieve from the crunchbase API the attributes we are interested in for each object type and the relationships we are going to fetch for each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relationships = ['founders', 'board_members_and_advisors', 'investors', 'headquarters', 'offices', 'categories', 'competitors', 'funding_rounds', 'acquisitions', 'acquired_by', 'ipo']\n",
    "\n",
    "relationships_infos = {}\n",
    "relationships_infos['person'] = ['bio', 'born_on', 'permalink']\n",
    "relationships_infos['address'] = ['city', 'country']\n",
    "relationships_infos['market'] = ['name']\n",
    "relationships_infos['product'] = ['name']\n",
    "relationships_infos['organization'] = ['permalink']\n",
    "relationships_infos['fundinground'] = ['announced_on', 'funding_type', 'money_raised_usd']\n",
    "relationships_infos['acquisition'] = ['announced_on']\n",
    "relationships_infos['ipo'] = ['went_public_on']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25171"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are retrieving only the companies founded before 2011\n",
    "len(focus_companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main script of this part. It extracts the features for each organization and requires several API calls per iteration. As a result, it takes a lot of time to run (around 5 hours for 1 000 companies).\n",
    "\n",
    "We first load the current json dictionary and then append to it the new companies we retrieved. We chose this structure of implementation to be able to run it in multiple times given the execution time needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6852\n"
     ]
    }
   ],
   "source": [
    "organizations_dict = {}\n",
    "# First load the organizations we already have and start from there\n",
    "with open('tmp/organizations_dict_to2010.json', 'r') as f:\n",
    "    organizations_dict = json.load(f)\n",
    "print len(organizations_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, permalink in enumerate(focus_companies.keys()[len(organizations_dict):len(focus_companies)]):\n",
    "    if i%50 ==0:\n",
    "        print 'iteration ', i, ' length is ', len(organizations_dict)\n",
    "    infos = {}\n",
    "    #permalink = organization['properties']['permalink']\n",
    "    for relation in relationships:\n",
    "        time.sleep(0.1)\n",
    "        url = u'https://api.crunchbase.com/v/3/organizations/{}/{}?user_key={}'.format(permalink, relation, user_key)\n",
    "        r_json = get_json(url)\n",
    "        if 'data' not in r_json:\n",
    "            continue\n",
    "        if 'items' in r_json['data']:\n",
    "            items = r_json['data']['items']\n",
    "        elif 'item' in r_json['data']:\n",
    "            items = r_json['data']['item']\n",
    "        else:\n",
    "            items = []\n",
    "        number_pages = r_json['data']['paging']['number_of_pages']\n",
    "        next_page_url = r_json['data']['paging']['next_page_url']\n",
    "        for j in xrange(2, number_pages + 1):\n",
    "            url = next_page_url[:-1]+str(j)+'&user_key='+user_key\n",
    "            r_json = get_json(url)\n",
    "            items += r_json['data']['items']\n",
    "        \n",
    "        relation_items = []\n",
    "        if type(items) is dict:\n",
    "            items = [items]\n",
    "        for item in items:\n",
    "            if 'relationships' in item:\n",
    "                for relationships_key in item['relationships']:\n",
    "                    relationships_key = relationships_key.lower()\n",
    "                    if relationships_key in relationships_infos:\n",
    "                        item_temp = {}\n",
    "                        for infos_key in relationships_infos[relationships_key]:\n",
    "                            item_temp[infos_key] = item['relationships'][relationships_key]['properties'][infos_key]\n",
    "                        relation_items.append(item_temp)\n",
    "            relationships_key = item['type'].lower()\n",
    "            if relationships_key in relationships_infos:\n",
    "                item_temp = {}\n",
    "                for infos_key in relationships_infos[relationships_key]:\n",
    "                    item_temp[infos_key] = item['properties'][infos_key]\n",
    "                relation_items.append(item_temp)\n",
    "        infos[relation] = relation_items\n",
    "    organizations_dict[permalink] = infos\n",
    "\n",
    "    with open('tmp/organizations_dict_to2010.json', 'w') as f:\n",
    "        json.dump(organizations_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This file is not present on the github repository because it's too heavy. \n",
    "with open('../organizations_total_json.json', 'r') as f:\n",
    "    organizations_tot = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407691\n",
      "408422\n"
     ]
    }
   ],
   "source": [
    "permalink2index = {v['properties']['permalink']:i for i, v in enumerate(organizations_tot)}\n",
    "print len(permalink2index)\n",
    "print len(organizations_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding the short description to the stored information (from the json with all the organizations)\n",
    "for o in organizations_dict:\n",
    "    if o in permalink2index:\n",
    "        i = permalink2index[o]\n",
    "        organizations_dict[o]['short_description'] = organizations_tot[i]['properties']['short_description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the dictionary is not in memory yet\n",
    "# with open('tmp/organizations_xlsx.json', 'r') as f:\n",
    "#    organizations_xlsx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the foundation date to the stored information (from the excel json)\n",
    "for o in organizations_dict:\n",
    "    if o in organizations_xlsx:\n",
    "        organizations_dict[o]['month_founded'] = organizations_xlsx[o]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tmp/organizations_dict_to2010.json', 'w') as f:\n",
    "    json.dump(organizations_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6852"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(organizations_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conclusion, our main learning from the scraping is that even though an API is available, you still need to dig deeper into the systems design. Besides, scrapping a lot of information over the wire is not immediate, it can take a lot of time especially if many calls are required and a cap is put on the bandwith.\n",
    "\n",
    "Building a robust and easy to use API is really important.\n",
    "We'll keep that advice in mind for the companies we are going to invest in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
